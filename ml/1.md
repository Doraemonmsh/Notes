## 线性回归
$$ Y=f(X)+\epsilon $$  
### input  
$ x_1^m $  
$ x_2^m $  
$ x_3^m $  
### output  
$ y^m $  
$ Y=M\beta\Rightarrow M^T Y=M^T M\beta\Rightarrow \beta=(M^T M)^{-1}M^{-1}Y $  
$$ y=ax_1+bx_2+cx_3+d+\epsilon $$ 
## 非线性回归
$$ y=ax_1^2+b\log x_2+ce^{x_3}+d $$  
### input
$$
\begin{pmatrix}
  x_1^2 \\
  \log x_2 \\
  e^{x_3} \\
  1
\end{pmatrix}
$$  
$$ \min\limits_{\beta}\quad\Vert X\beta-Y\Vert^2+\lambda\Vert\beta\Vert^2 $$  
### 凸函数
$$ f(\lambda\beta_1+(1-\lambda)\beta_2)\ge\lambda f(\beta_1)+(1-\lambda)f(\beta_2) $$  
### 伪梯度
$ y=\vert x_1\vert+\vert x_2\vert+\vert x_3\vert $  
$ g=\operatorname{sgn}(x_1,x_2,x_3) $  
### 求解
$$ \min\limits_{\beta}\quad\Vert X\beta-Y\Vert_2^2+\lambda\Vert\beta\Vert_2^2 $$  
- 求导  
  $$ \frac{\partial(X\beta-Y)^T(X\beta-Y)+\lambda\beta^T\beta}{\partial\beta} $$  
  $$ (X\beta-Y)^T X+\lambda\beta^T=0 $$  
  $$ (X^T X+\lambda I)^T\beta=X^T Y $$  
### 矩阵乘法
$$ C=A\cdot B\Rightarrow C_{ij}=\sum_{k}A_{ik}B_{kj} $$  
$$ D=A\cdot B\cdot C\Rightarrow D_{ij}=\sum_{k_1}\sum_{k_2}A_{i k_1}B_{k_1 k_2}C_{k_2 j} $$  
### 矩阵求导
$$ f(X)=AX+b $$  
- 变量是向量，函数是向量  
  $$ \frac{\partial(f_1,\dots,f_m)}{\partial(x_1,\dots,x_n)} $$  
  $$ D_{ij}=\frac{\partial f_i}{\partial x_j}=A_{ij} $$  
  则  
  $$ \frac{\partial f}{\partial X}=A $$  
  A是矩阵，x是实数$ A=f(x) $  
  则  
  $$ \frac{\partial A^{-1}}{\partial x}=-A^{-1}\frac{\partial A}{\partial x}A^{-1} $$  
- 变量是矩阵，函数是标量  
  $$
  \frac{\partial f}{\partial x}=
  \begin{bmatrix}
  \frac{\partial f}{\partial x_{11}} & \cdots & \frac{\partial f}{\partial x_{1n}} \\
  \vdots & \ddots & \vdots \\
   \frac{\partial f}{\partial x_{n1}} & \cdots & \frac{\partial f}{\partial x_{nn}}
  \end{bmatrix}
  $$  
- 迹
  `迹 tr`是矩阵对角线元素之和  
  $$ \frac{\partial tr[AXB]}{\partial x} $$  
  $$ [AXB]_{hk} = \sum_{i}\sum_{j}A_{hi}X_{ij}B_{jk} $$  
  A是个矩阵
  A的迹 $ tr[A]=\sum_{i=1}^{n}a_{ii} $
  $$ tr[AXB]=\sum_{h}\sum_{i}\sum_{j}A_{hi}X_{ij}B_{jh} $$  
  $$ \because\quad\frac{\partial tr[AXB]}{\partial x_{ij}}=\sum_{h}A_{hi}B_{jh}=\sum_{h}B_{jh}A_{hi}=[BA]_{ji}=[(BA)^T]_{ij} $$  
  $$ \therefore\quad\frac{\partial tr[AXB]}{\partial x}=A^T B^T $$  
  $$ \frac{\partial tr[AX^T B]}{\partial x}=\frac{\partial tr[B^T XA^T]}{\partial x}=BA $$  
  $$ \frac{\partial tr[AXBXC]}{\partial x}=A^T C^T X^T B^T+B^T X^T A^T C^T $$  
- 范式  
  $$ \Vert A\Vert_F^2=\sum_{i}\sum_{j}a_{ij}^2 $$  
  $$ [AA^T]_{ii}=\sum_{k}A_{ik}^2 $$  
  $$ tr[AA^T]=\sum_{i}\sum_{j}A_{ij}^2=\Vert A\Vert_F^2 $$  
  $$ \frac{\partial\Vert B-XA\Vert_F^2}{\partial x}=\frac{\partial tr[(B-XA)^T(B-XA)]}{\partial x} $$  
  $$ =\frac{\partial tr[B^T B+A^T X^T XA-A^T X^T B-B^T XA]}{\partial x} $$  
  $$ =2XAA^T-2BA^T $$  
  令$ 2XAA^T-2BA^T=0 $得：  
  $$ X=BA^T(AA^T)^{-1} $$  
  $$  $$  
  $$  $$  
  $$  $$  
  $$  $$  
  $$  $$  

## logistic regression $ \Rightarrow $ 分类
$$ q(X=x)=P(Y=1|X=x) $$  
$$ \frac{q}{1-q}\in(0,+\infty)\quad\log\frac{q}{1-q}\in(-\infty,+\infty) $$  
$$ \log\frac{q}{1-q}=ax_1+bx_2+cx_3+d=\beta x $$  
$$ q=\frac{e^{\beta x}}{1+e^{\beta x}} $$  
$$ P(Y_1=y_1,\dots,Y_n=y_n|X_1=x_1,\dots,X_n=x_n) $$
$ \displaystyle=\prod_{i:y_i=1}q(x_i,\beta)\cdot\prod_{i:y_i=0}(1-q(x_i,\beta)) $  
$ \displaystyle=\sum_{i=1}^n y_i\log q(x_i;\beta)+(1-y_i)\log (1-q(x_i;\beta)) $  
$ \displaystyle=\sum\{y_i\beta^T x_i-\log(1+e^{\beta^T x_i})\} $  

***
***
***
***
***
***
***
***
***
***
***
***
***
***